# ğŸ¦ Loan Prediction

This project presents a complete machine learning pipeline for predicting loan eligibility. The solution was developed as part of the **Analytics Vidhya Loan Prediction Practice Problem**.

ğŸ”— Hackathon Link:
[Click here](https://www.analyticsvidhya.com/datahack/contest/practice-problem-loan-prediction-iii/)

ğŸ“– Non-Technical Case Study (Notion Version):
[Click here](https://absorbing-sight-c8d.notion.site/Loan-Eligibility-Prediction-306705565b44803490aaebb09ee5c3de)

---

## ğŸ“Œ Problem Statement

Dream Housing Finance provides home loans across urban, semi-urban, and rural areas. Customers apply online, and the company must assess their eligibility before approving the loan.

The objective of this project is to **automate the loan eligibility process in real-time** using customer information provided during the application process, including:

* Gender
* Marital Status
* Education
* Number of Dependents
* Applicant Income
* Coapplicant Income
* Loan Amount
* Loan Term
* Credit History
* Property Area

Using historical data, we build predictive models to identify applicants likely to be approved for a loan.

---

## ğŸ’¡ Hypothesis Generation

Based on domain knowledge, the following factors were hypothesised to influence loan approval:

* **Applicant Income:** Higher income increases approval likelihood.
* **Credit History:** Positive credit history strongly increases approval chances.
* **Loan Amount:** Smaller loan amounts are more likely to be approved.
* **Loan Term:** A shorter loan duration may improve approval probability.
* **EMI:** Lower monthly repayment burden increases chances of approval.

These hypotheses were tested using exploratory data analysis and machine learning models.

---

## ğŸ“Š Exploratory Data Analysis

Key insights were derived through structured data exploration.

### ğŸ“Œ Target Distribution

<p align="center">
  <img src="images/loan_status_distribution.png" width="600"/>
</p>
<p align="center"><i>Figure 1: Distribution of Approved vs Rejected Loan Applications.</i></p>

---

### ğŸ“Œ Loan Amount Distribution (Before & After Log Transformation)

<p align="center">
  <img src="images/loan_amount_dist_before.png" width="600"/>
</p>
<p align="center"><i>Figure 2: Distribution of Loan Amount Variable Before Log Transformation.</i></p>

<p align="center">
  <img src="images/loan_amount_dist_after.png" width="600"/>
</p>
<p align="center"><i>Figure 3: Distribution of Loan Amount Variable After Outlier Removal and Log Transformation.</i></p>

---

### ğŸ“Œ Correlation Heatmap

<p align="center">
  <img src="images/correlation_matrix.png" width="600"/>
</p>
<p align="center"><i>Figure 4: Correlation Heatmap Between Numerical Features.</i></p>

---

## ğŸ›  Feature Engineering

Created new features based on financial reasoning:

* **Total Income**
* **EMI (Equated Monthly Instalment)**
* **Balance Income**
* Log transformations for skewed variables

These features improved model performance by approximately **2â€“3%** compared to the baseline model.

### ğŸ“Œ Balance Income Distribution

<p align="center">
  <img src="images/balance_income_dist.png" width="600"/>
</p>
<p align="center"><i>Figure 5: Balance Income variable distribution.</i></p>

---

## ğŸ¤– Model Building

Models implemented:

* Logistic Regression (Baseline)
* Logistic Regression with Stratified K-Fold Cross Validation
* Decision Tree
* Random Forest
* XGBoost

---

## ğŸ“ˆ Model Comparison

| Model               | Validation Accuracy | Submission Accuracy |
| ------------------- | ------------------- | ------------------- |
| Logistic Regression | 0.796               | 0.792               |
| Decision Tree       | 0.707               | 0.653               |
| Random Forest       | 0.809               | 0.778               |
| XGBoost             | 0.749               | 0.722               |

### ğŸ“Œ Model Performance Comparison

<p align="center">
  <img src="images/accuracy_comparison.png" width="600"/>
</p>
<p align="center"><i>Figure 6: Comparison between Validation Accuracy (test dataset) and Submission Accuracy (hackathon dataset).</i></p>

---

## ğŸ† Final Model Selection

**Logistic Regression** was selected as the final model due to:

* Strong and stable generalisation performance
* Minimal overfitting gap between validation and submission
* Simplicity and interpretability
* Computational efficiency

---

## ğŸ” Feature Importance

Tree-based models showed:

* **Credit History** as the most influential feature
* Followed by Balance Income, EMI, and Total Income

### ğŸ“Œ Feature Importance Plot

<p align="center">
  <img src="images/feature_importance_randomforest.png" width="600"/>
</p>
<p align="center"><i>Figure 7: Feature Importance Score Plot.</i></p>

---

## âš™ï¸ Project Setup

### 1ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate
```

### 2ï¸âƒ£ Install Requirements

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ .gitignore Includes

```
venv/
__pycache__/
.ipynb_checkpoints/
.env
.DS_Store
```

---

## ğŸ“ Repository Structure

```
Loan_Prediction/
â”‚
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ data.ipynb
â”‚   â”œâ”€â”€ submission_logistic.csv
â”‚   â”œâ”€â”€ submission_logistic_kfolds_validation.csv
â”‚   â”œâ”€â”€ submission_logistic_2.csv
â”‚   â”œâ”€â”€ submission_decision_tree.csv
â”‚   â”œâ”€â”€ submission_random_forest.csv
â”‚   â”œâ”€â”€ submission_xgboost.csv
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ sample_submission_49d68Cx.csv
â”‚   â”œâ”€â”€ test_lAUu6dG.csv
â”‚   â”œâ”€â”€ train_ctrUa4K.csv
â”‚
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ target_distribution.png
â”‚   â”œâ”€â”€ income_distribution.png
â”‚   â”œâ”€â”€ correlation_heatmap.png
â”‚   â”œâ”€â”€ balance_income.png
â”‚   â”œâ”€â”€ model_comparison.png
â”‚   â”œâ”€â”€ feature_importance.png
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ“‚ Submission Files

* `submission_logistic.csv` â€“ Baseline Logistic Regression
* `submission_logistic_kfolds_validation.csv` â€“ Logistic Regression with K-Folds
* `submission_logistic_2.csv` â€“ Logistic Regression after feature engineering
* `submission_decision_tree.csv` â€“ Decision Tree results
* `submission_random_forest.csv` â€“ Random Forest results
* `submission_xgboost.csv` â€“ XGBoost results

---

## ğŸš€ Key Learnings

* Feature engineering significantly improves model performance.
* Financial burden metrics outperform raw income.
* Simpler models can outperform complex ensemble methods.
* Cross-validation prevents overfitting and ensures stability.

---

## ğŸ§  Technologies Used

* Python
* Pandas
* NumPy
* Matplotlib
* Scikit-learn
* XGBoost

---

## ğŸ“Œ Author

Hamza Latif
